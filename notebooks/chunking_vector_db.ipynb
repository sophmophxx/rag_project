{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11accee7",
   "metadata": {},
   "source": [
    "\n",
    "# Semantische Suche in arXiv-Artikeln mit LangChain\n",
    "\n",
    "In dieser √úbung verwenden wir arXiv, um echte wissenschaftliche Abstracts abzurufen, sie zu chunken und in einer Vektordatenbank zu speichern; und schlie√ülich darauf semantische Suchen durchzuf√ºhren.\n",
    "\n",
    "Dabei lernen Sie: \n",
    "- arXiv-Abstracts abzurufen und zu verarbeiten  \n",
    "- Texte in Chunks aufzteilen  \n",
    "- Embeddings zu erzeugen   \n",
    "- eine Vektordatenbank aufbauen (Chroma)  \n",
    "- semantische √Ñhnlichkeitssuche durchzuf√ºhren  \n",
    "\n",
    "\n",
    "## arXiv\n",
    "arXiv (ausgesprochen **archive**) ist eine Plattform f√ºr wissenschaftliche Preprints.\n",
    "\n",
    "Forschende aus Bereichen wie Physik, Informatik, Mathematik, KI oder Statistik ver√∂ffentlichen dort ihre Arbeiten, bevor sie in Fachzeitschriften erscheinen.\n",
    "\n",
    "- offen, kostenlos und ohne Login nutzbar.\n",
    "- strukturierte Metadaten (Titel, Abstract, Autor\\*innen, Ver√∂ffentlichungsdatum)\n",
    "- API-Zugang, dadurch nutzbar f√ºr Text- und Datenanalysen\n",
    "\n",
    "\n",
    "## LangChain\n",
    "LangChain ist ein Python-Framework zur Entwicklung von Anwendungen, die mit gro√üen Sprachmodellen (LLMs) oder semantischer Textverarbeitung arbeiten.\n",
    "Es stellt Bausteine bereit f√ºr:\n",
    "\n",
    "- Datenverarbeitung: Laden, Aufteilen und Strukturieren von Texten\n",
    "- Embeddings: Umwandlung von Text in numerische Vektoren zur semantischen Analyse\n",
    "- Vektordatenbanken: Speicherung und Wiederfinden semantisch √§hnlicher Inhalte\n",
    "- (optional) LLMs: Anbindung von Modellen wie GPT, Mistral oder Claude f√ºr Textgenerierung und Dialoge\n",
    "\n",
    "In dieser √úbung konzentrieren wir uns ausschlie√ülich auf den Retrieval-Teil ‚Äì also das Chunken, Erstellen von Embeddings und semantische Suchen, nicht auf die Textgenerierung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2762641",
   "metadata": {},
   "source": [
    "## 0. Installation ben√∂tigter Bibliotheken\n",
    "\n",
    "- Aktivieren Sie Ihre `venv`\n",
    "- Aktivieren Sie die ben√∂tigten Bibliotheken: `pip install -U langchain langchain-community langchain-text-splitters langchain-huggingface chromadb sentence-transformers torch arxiv tqdm`\n",
    "- Aktualisieren Sie Ihre `requirements.txt`\n",
    "- Nach der Installation m√ºssen Sie den Kernel des Notebooks ggf. neu starten, damit die neuen Bibliotheken verf√ºgbar sind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc1547",
   "metadata": {},
   "source": [
    "## 1. Data Collection mit der arXiv API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b0323",
   "metadata": {},
   "source": [
    "1. √úberlegen Sie sich einen Suchbegriff und definieren Sie eine entsprechende Variable. \n",
    "2. √úberlegen Sie, wieviele Treffer Sie erhalten m√∂chten und definieren Sie eine entsprechende Variable. \n",
    "3. √úbergeben Sie die beiden Variablen als Parameter `query` und `max_results` an das untenstehende Search()-Objekt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d003347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ...\n",
    "# ...\n",
    "# search = arxiv.Search(..., ..., sort_by=arxiv.SortCriterion.Relevance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38131899",
   "metadata": {},
   "source": [
    "4. F√ºhren Sie die Suche aus und lassen Sie sich die Trefferliste mit dem vorhandenen Code ausgeben. \n",
    "5. Bonus: √Ñndern Sie die Sortierung der Ergebnisse von `Relevance` auf `SubmittedDate` und f√ºhren Sie die Suche erneut aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21289c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j9/lnrs75px2vvdhgh2j143swp40000gn/T/ipykernel_10477/1247162351.py:3: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Artikel geladen.\n",
      "\n",
      "üìÑ Trustworthy and Efficient LLMs Meet Databases\n",
      "In the rapidly evolving AI era with large language models (LLMs) at the core,\n",
      "making LLMs more trustworthy and efficient, especially in output generation\n",
      "(inference), has gained significant attention. This is to reduce plausible but\n",
      "faulty LLM output...\n",
      "\n",
      "üìÑ Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications\n",
      "Large Language Models (LLMs) have become widely adopted recently. Research\n",
      "explores their use both as autonomous agents and as tools for software\n",
      "engineering. LLM-integrated applications, on the other hand, are software\n",
      "systems that leverage an LLM t...\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for result in search.results():\n",
    "    docs.append({\n",
    "        \"title\": result.title,\n",
    "        \"summary\": result.summary,\n",
    "        \"url\": result.entry_id,\n",
    "        \"published\": result.published\n",
    "    })\n",
    "\n",
    "print(f\"{len(docs)} Artikel geladen.\")\n",
    "for d in docs[:2]:\n",
    "    print(f\"\\nüìÑ {d['title']}\\n{d['summary'][:250]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc73ec62",
   "metadata": {},
   "source": [
    "## 2. Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeab3e4",
   "metadata": {},
   "source": [
    "1. Betrachten Sie den folgenden Code: Welche Chunking-Methode wird hier eingesetzt? \n",
    "2. Passen Sie den Code so an, dass Sliding Window ber√ºcksichtigt wird. \n",
    "3. Experimentieren Sie mit verschiedenen Werten f√ºr die Gr√∂√üe der Chunks und des Sliding Window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "204945f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 Chunks erzeugt.\n",
      "\n",
      "--- Chunk 1 ---\n",
      "In the rapidly evolving AI era with large language models (LLMs) at the core,\n",
      "making LLMs more\n",
      "\n",
      "--- Chunk 2 ---\n",
      "trustworthy and efficient, especially in output generation\n",
      "(inference), has gained significant\n",
      "\n",
      "--- Chunk 3 ---\n",
      "attention. This is to reduce plausible but\n",
      "faulty LLM outputs (a.k.a hallucinations) and meet the\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# In LangChain-Dokumente umwandeln\n",
    "documents = [\n",
    "    Document(page_content=d[\"summary\"], metadata={\"title\": d[\"title\"], \"url\": d[\"url\"]})\n",
    "    for d in docs\n",
    "]\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator = ' ',\n",
    "    chunk_size=100, \n",
    "    chunk_overlap=0\n",
    "    )\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"{len(chunks)} Chunks erzeugt.\")\n",
    "\n",
    "for i, c in enumerate(chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(c.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dff359",
   "metadata": {},
   "source": [
    "## 3. Embeddings erzeugen und Vektordatenbank erstellen\n",
    "1. W√§hlen Sie ein HggingFace-Embedding-Modell aus (z.B. eines von denen, die wir uns bereits gemeinsam angeschaut haben). \n",
    "2. Nutzen Sie das Modell, um Embeddings f√ºr die Chunks zu erzeugen und eine Vektordatenbank zu erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c27dacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elisabeth/repos/rag_course/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vektordatenbank erstellt.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# model_name = ... \n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "db = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=\"../../data/arxiv_db\")\n",
    "\n",
    "print(\"Vektordatenbank erstellt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f536d1",
   "metadata": {},
   "source": [
    "## 4. Semantische Suche in der Vektordatenbank\n",
    "1. √úberlegen Sie sich eine Frage oder Phrase, mit der Sie nach √§hnlichen Dokumenten in der Vektordatenbank suchen und schauen Sie sich die Ergebnisse an. \n",
    "2. Experimentieren Sie mit verschiedenen Queries und Werten f√ºr `k`. \n",
    "3. Wie k√∂nnte man Metadaten (z. B. Jahr, Autor:innen) f√ºr die Suche nutzen? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ae7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Treffer: A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends\n",
      "performance\n",
      "compared to general LLMs. We aim to address three questions: (1) What LLMs\n",
      "Quelle: http://arxiv.org/abs/2311.10372v2\n",
      "\n",
      " Treffer: A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends\n",
      "performance\n",
      "compared to general LLMs. We aim to address three questions: (1) What LLMs\n",
      "Quelle: http://arxiv.org/abs/2311.10372v2\n",
      "\n",
      " Treffer: A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends\n",
      "performance\n",
      "compared to general LLMs. We aim to address three questions: (1) What LLMs\n",
      "Quelle: http://arxiv.org/abs/2311.10372v2\n"
     ]
    }
   ],
   "source": [
    "# query = ...\n",
    "# k = ...\n",
    "results = db.similarity_search(query, k)\n",
    "\n",
    "for hit in results:\n",
    "    print(f\"\\n Treffer: {hit.metadata['title']}\")\n",
    "    print(hit.page_content)\n",
    "    print(\"Quelle:\", hit.metadata[\"url\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc75fe02",
   "metadata": {},
   "source": [
    "## 5. Erweiterungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdad527",
   "metadata": {},
   "source": [
    "### 1. Mehr Daten\n",
    "- Holen Sie sich eine gr√∂√üere Menge an Dokumenten (z.B. `max_results=100`) und erstellen Sie damit eine Vektordatenbank. \n",
    "- Holen Sie sich Artikel zu **verschiedenen** Suchbegriffen und kombinieren Sie diese in einer Vektordatenbank. \n",
    "\n",
    "### 2. Andere Chunking-Methode\n",
    "- Informieren Sie sich √ºber andere M√∂glichkeiten des Chunkings mit Hilfe der LangChain-Docs: https://docs.langchain.com/oss/python/integrations/splitters\n",
    "- Probieren Sie mindestens eine weiter Chunking-Methode aus und analysieren Sie, was sich ver√§ndert. \n",
    "- Welche Methode w√ºrdem Sie f√ºr arXiv-Abstracts empfehlen und warum?\n",
    "- W√§re es auch eine M√∂glichkeit, gar nicht zu chunken? Begr√ºnden Sie.  \n",
    "\n",
    "### 3. Verfeinerung der Suche\n",
    "- Filtern Sie die Ergebnisse nach Erscheinungsjahr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
